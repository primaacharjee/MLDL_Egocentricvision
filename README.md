# egocentricvision
Egocentric videos, which capture everyday activities from a first-person perspective, offer unique challenges and opportunities for leveraging video contents through Natural Language Queries. Traditional models for video segment localization output time intervals in response to queries, requiring viewers to watch these segments to obtain the actual answers. Differently, we propose a pipeline in which relevant video segments are identified using advanced video segment localization models, and then these segments are processed by a VideoQA (Video Question Answering) Model to obtain natural language answers. This approach efficiently handles long videos by focusing computational resources on relevant segments, thus processing queries efficiently. The proposed pipeline effectively addresses the challenge, exhibiting an average improvement of +3.2\% over the segment localization baseline and demonstrating optimal performance in generating natural language responses.
